import re
import os
import datetime
import json

# from dotenv import load_dotenv
# load_dotenv()

from util import generatedExcelFile, getDayAndWeek, sendEmail, remap_weekday, find_files_created_days_ago

now = datetime.datetime.now()
sessionDate = now.strftime("%Y-%m-%d")

now = datetime.datetime.now()

weekday = remap_weekday(now.weekday())

#Check if current day is Process day(Sunday or Monday), Sun = 1, Monday = 2
if(weekday != 1 and weekday != 2):
    print("Exited!. Current day is not proceesing day(Sunday or Monday)")
    exit()

# if now is sunday, process all file generated in the saturday(previous day)
if weekday == 1:
    print("Today is Sunday, will process GTD5 files generated on last Saturday")

# if now is monday, process all file generated in the sunday(previous day)
if weekday == 2:
    print("Today is Monday, will process GTD5 files generated on last Sunday")

daysAgo = 1

# Process files generated a day ago 
[dayToBeProcess, weekToBeProcess] = getDayAndWeek(now - datetime.timedelta(days=daysAgo))
# [day, week] = getDayAndWeek(now)
# [day, week] = getDayAndWeek(now + datetime.timedelta(days=12))

#Check if current day is not saturday or sunday, Sun = 1, Sat = 7
# if(day != 1 and day != 7):
#     print("Exited!. Current day is not saturday nor sunday")
#     exit()

rawFilesDir = "raw_files"

print(os.listdir(rawFilesDir))

# List all files to be processed
filesGeneratedFewDaysAgo = find_files_created_days_ago(rawFilesDir, daysAgo)

print(f"Files generated by Resolve {daysAgo} day(s) ago")
print(filesGeneratedFewDaysAgo)

groupKey = "w" + str(weekToBeProcess) + "_d" + str(dayToBeProcess) 

print("Started...")

print(f"Week and Day : {groupKey}")

# Expected files to be process each week
# These will be used to check against the files uploaded
# w_#_# - w_(week#)_(day#)
sitesGroups = {
    "w1_d7" : ["PTAIBC01DS1","NLSNBC01DS1","FSJNBC01DS2","VANCBC03DS1","HANYBC01DS1","LNGLBC01DS1"], # week 1, Sat
    "w1_d1" : ["BNVTPQXQDS1","MTMGPQXQDS1","STHLPQXQDS1","MATNPQXQDS1","BRKSAB01CG1","EDSOAB01CG0","CLGRAB04CG0"], # week 1, Sun
    "w2_d7" : ["PTCMBC02DS2","WVCRBC01DS1","NVCRBC01DS1","NNIMBC03DS1","LEDCAB01CG1","SYPLAB02CG1"], # week 2, Sat
    "w2_d1" : ["SDNYBC01DS1","IVMRBC01DS1","KLWNBC02DS2","KMLPBC02DS1","VCTABC06DS1","WLLKBC01DS1","TRRCBC02DS1"], # week 2, Sun
    "w3_d7" : ["BNBYBC01DS1","VERNBC01DS1","NNIMBC01DS1","WLOCAB01CG1","CLWKBC01DS1","CRBKBC01DS1"], # week 3, Sat
    "w3_d1" : ["GBSNBC01DS1","VCTABC05DS1","DWCKBC01DS1","TRALBC01DS1","CRTYBC01DS1","CBRVBC01DS1"], # week 3, Sun
    "w4_d7" : ["PWRVBC01DS1","ARGVBC01DS1","CQTLBC01DS1","ABFDBC01DS2","PNTNBC02DS1","SRRYBC01DS2"], # week 4, Sat
    "w4_d1" : ["DELTBC01DS1","RCMDBC02DS2","WHRKBC01DS2","VCTABC03DS2","SLAMBC01DS1","KMLPBC01DS2"], # week 4, Sun
}

tcuList = []
fileList = os.listdir(rawFilesDir)

_sitesFileStatus = []

# sites = sitesGroups["w3_d1"] 
sites = sitesGroups[groupKey]

print(f"Sites : {sites}")

def logSiteStatus(sList:list, f:str, dr:bool, dc:bool, fs, r=""):
    sList.append({
        "name": f,
        "dataReceived": str(dr),
        "dataCalculated": str(dc),
        "fileSize": fs,
        "remarks": r
    })

def processLine(fileName:str, line:str, searchStr:str, addPos:int, tcuList:list):
    search = re.search(searchStr, line)
    if search :
        position = search.start()
        stringTcu = line[position : position + addPos]
        darkCount = "WC"
        if "TDS0299" in line: darkCount = "DD"
        tcuList.append(fileName + "_" + stringTcu + "_" + darkCount)

def emailReport(body, gtd5FileGenerationDay, sessionDate):
    weekDays = {
        "1" : "Sunday",
        "7" : "Saturday"
    }

    day = weekDays[f"{gtd5FileGenerationDay}"]

    subject = f"[{day}] GTD5 Customer Count Process Result {sessionDate}"

    subject = f"[TEST] {subject}"

    recepient = "eric.sangabriel@telus.com"
    cc = ["eric.sangabriel@telus.com"]

    sendEmail(subject, body, recepient, cc)

def main():

    filesToBeProcessed = []

    print(filesGeneratedFewDaysAgo)

    # Filters files based in the predefined sites to be processed in a certain week and day, see siteGroup
    for filePath in filesGeneratedFewDaysAgo:
        for site in sites:
            if site in filePath:
                print("site in path")
                filesToBeProcessed.append({
                    "filePath": filePath,
                    "site" : site
                })
                break

    print(f"Files to be processed : {len(filesToBeProcessed)}")

    for fileToBeProcessed in filesToBeProcessed:

        filePath = fileToBeProcessed["filePath"]
        site = fileToBeProcessed["site"]

        try:
            fileStat= os.stat(filePath)
            fileSize = str(round(fileStat.st_size / (1024 * 1024), 2)) + "MB"

            # print(f"{site} size : {fileSize}")

            isFileComplete = False
            
            file = open(filePath, "r")
            # check first if the file is complete by looking for "RANGE PROCESSING COMPLETE"
            for line in file: 
                if "RANGE PROCESSING COMPLETE" in line:
                    isFileComplete = True
                    break
            file.close()
            
            # print(f"{site} is complete? : {isFileComplete}")

            if isFileComplete:
                # sites.remove(fileName) #Removes filename from the list
                
                # Reopens file for the actual parsing
                file = open(filePath, "r")
                for line in file:
                    if " DN " in line and " TDS" in line:
                        if "TCU" in line:
                            processLine(site, line, "TCU\d\d\d\.\d\d\d\d", 11, tcuList)
                        elif "RSU" in line:
                            processLine(site, line, "RSU\d\d\d\.\d\d\d\d", 11, tcuList)
                        elif "RLU" in line:
                            processLine(site, line, "RLU\d\d\d\.\d\d\d\d", 6, tcuList)
                        elif "MXU" in line:
                            processLine(site, line, "MXU\d\d\d\.\d\d\d\d", 6, tcuList)
                        else :
                            None
                logSiteStatus(_sitesFileStatus, site, True, True, fileSize, "OK")
            
            else :
                logSiteStatus(_sitesFileStatus, site, True, True, fileSize, "Incomplete File, no `RANGE PROCESSING COMPLETE` found")
                
        except Exception:
            print("ERROR")
            f = open('error.log', 'w')
            f.write('An exceptional thing happed during opening '+ site +' - %s' % e)
            f.close()
            logSiteStatus(_sitesFileStatus, site, True, False, "", "Error when opening the file")
    
    print("Read done")

    # if len(sites) > 0:
    #     for site in sites:
    #         logSiteStatus(_sitesFileStatus, site, False, False, "", "No raw file found")

    # sitesFileStatus = { "sites" : _sitesFileStatus }



    if len(filesToBeProcessed) > 0:

        tcuList.sort()

        print("Processing counts...")
        result = dict(zip(list(tcuList),[list(tcuList).count(i) for i in list(tcuList)]))

        # y = json.dumps(sitesFileStatus, indent=2)
        # print(y)

        # sitesFileStatus TODO - Log this
        print("Generatin xlsx file...")
        generatedExcelFile(result, sessionDate)

        body = ""
        for status in _sitesFileStatus:
            site = status["name"]
            remarks = status["remarks"]
            fileSize = status["fileSize"]

            body = f"{body}\n\n{site} | {fileSize} |  {remarks}"

        print(f"Body : {body}")

        emailReport(body=body, gtd5FileGenerationDay=dayToBeProcess, sessionDate=sessionDate)

    print("Done!")


main()

# TODO - Copy generated csv/xlsx to /devops/projects/Customer_Count/Voice/GTD5 dir?
# TODO - Compress raw files and store the somewhere(maybe GDrive?), delete raw files after processing?
# TODO - Create endpoint that will be triggered by Resolve?
# TODO - Cron triggered?






